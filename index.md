<center><b>Security and Privacy in Machine Learning</b></center>
<center>Sharif University of Technology, Iran</center>
<center>CE Department</center>
<center>Spring 2023</center>


&nbsp;&nbsp;&nbsp;


_Welcome_ to the public page for the course on Security and Privacy in Machine Learning (SPML). The main objectives of the course are to introduce students to the principles of security and privacy in machine learning. The students become familiar with the vulnerabilities of machine learning in the training and inference phases and the methods to improve the robustness and privacy of machine learning models.



**Course Logistics**

   * **Time:** Sat. & Mon. 13:30 - 15:00
   * **Location:** CE-202 & [vc.sharif.edu/ch/amsadeghzadeh](https://vc.sharif.edu/ch/amsadeghzadeh)
   * **Contact:** Announcements and all course-related questions will happen on the [Quera](https://quera.org/course/add_to_course/course/13164/) forum. 
     * All official announcements and communication will happen over Quera.
     * For external enquiries, emergencies, or personal matters that you don't wish to put in a private post, you can email me at amsadeghzadeh_at_gmail.com



**Instructor**

&nbsp;&nbsp;&nbsp;_Amir Mahdi Sadeghzadeh_  
&nbsp;&nbsp;&nbsp;Office: CE-501 (DNSL)  
&nbsp;&nbsp;&nbsp;Office Hours: By appointment (through Email)  
&nbsp;&nbsp;&nbsp;Email: [amsadeghzadeh_at_gmail.com](mailto:amsadeghzadeh_at_gmail.com)  
&nbsp;&nbsp;&nbsp;URL: [amsadeghzadeh.github.io](https://amsadeghzadeh.github.io)  



**Course Staff**

* _Mahdi Ghaznavi_ (Head Course Assistant) - Email: [ghaznavi.mahdi_at_gmail.com](mailto:ghaznavi.mahdi_at_gmail.com)
* _Zeinab Golgooni_ (Course Assistant) - Email: [z.golgooni_at_gmail.com](mailto:z.golgooni_at_gmail.com)
* _Elahe Farshadfar_ (Course Assistant) - Email: [elahefarshadfar1377_at_gmail.com](mailto:elahefarshadfar1377_at_gmail.com)
* _Mohammad Reza Kazemi_ (Course Assistant) - Email: [mokazemi98_at_gmail.com](mailto:mokazemi98@gmail.com)
* _Hamid Dashtbani_ (Course Assistant) - Email: [hamiddb77_at_gmail.com](mailto:hamiddb77@gmail.com)



**Course Pages** 

* [spml-sut.github.io](spml-sut.github.io) -> Course information, syllabus, and materials.
* [Quera](https://quera.org/course/add_to_course/course/13164/) (Get the password from course staff) -> Announcements, assignments, and all course-related questions.



**Main References** 

The main references for the course are many research papers in top-tier conferences and journals in computer security (SP, CCS, Usenix Security, EuroSP) and machine learning (NeurIPS, ICLR, ICML, CVPR, ECCV). Three following books are used for presenting background topics in machine learning and deep learning in
the first part of the course.

-   [Christopher M. Bishop, *Pattern Recognition and Machine Learning*,
    Springer,
    2006.](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

-   [Ian Goodfellow, *Deep Learning*, MIT Press,
    2016.](https://www.deeplearningbook.org/)

-   [Aston Zhang, Dive into Deep Learning, 2020 ](http://d2l.ai/)



**Grading Policy**

Assignments (30%), Mid-term (and Mini-exam) (20%), Papers review and presentation(20%), and Final (30%).



**Course Policy**

-   This course considers topics involving personal and public privacy
    and security. As part of this investigation we will cover
    technologies whose abuse may infringe on the rights of others. As an
    instructor, I rely on the ethical use of these technologies.
    Unethical use may include circumvention of existing security or
    privacy measurements for any purpose, or the dissemination,
    promotion, or exploitation of vulnerabilities of these services.
    Exceptions to these guidelines may occur in the process of reporting
    vulnerabilities through public and authoritative channels. Any
    activity outside the letter or spirit of these guidelines will be
    reported to the proper authorities and may result in dismissal from
    the class. When in doubt, please contact the instructor for advice. **Do not**
    undertake any action which could be perceived as technology misuse
    anywhere and/or under any circumstances unless you have received
    explicit permission from Dr. Sadeghzadeh.



**Academic Honesty** 

[Sharif CE Department Honor Code](https://wiki.ce.sharif.edu/%D8%A2%DB%8C%DB%8C%D9%86_%D9%86%D8%A7%D9%85%D9%87/%D8%A2%D8%AF%D8%A7%D8%A8_%D9%86%D8%A7%D9%85%D9%87_%D8%A7%D9%86%D8%AC%D8%A7%D9%85_%D8%AA%D9%85%D8%B1%DB%8C%D9%86_%D9%87%D8%A7%DB%8C_%D8%AF%D8%B1%D8%B3%DB%8C) (please read it carefully!)



**Homework Submission**

Submit your answers in .pdf or .zip file in course page on Quera website, with the following format:
HW[HW#]-[FamilyName]-[std#] (For example HW3-Hoseini-401234567)



**Late Policy**

* All students have 21 free late days for the assignments.
* You may use up to 7 late days per assignment with no penalty.
* Once you have exhausted your free late days, we will deduct a late penalty of 20% per additional late day.


&nbsp;&nbsp;&nbsp;

&nbsp;&nbsp;&nbsp;


| # | Date  | Topic             | Content                                    | Lecture | Reading                                                                                                                                                                                                                                                                               | HWs |
|---|-------|-------------------|--------------------------------------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----|
| 1 | 11/17 | Course Intro.     | The scope and contents of the course       | [Lec1](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec1.pdf)    | [Towards the Science of Security and Privacy in Machine Learning](https://arxiv.org/abs/1611.03814)                                                                                                                                                                           |     |
| 2 | 11/22 | Public Holiday           |                                            |         |                                                                                                                                                                                                                                                                                       |     |
| 3 | 11/24 | Machine Learning  | ML Intro., Perceptron, Logistic regression | [Lec2](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec2.pdf)    | [Pattern Recognition and Machine Learning Ch.1 & Ch.4](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf) <br> [Deep Learning Ch.5](https://www.deeplearningbook.org/) |     |
| 4 | 11/29 | Public Holiday           |                                            |         |                                                                                                                                                                                                                                                                                       |     |
| 5 | 12/1  | Linear Classifier | Gradient descent, Regularization        | [Lec3](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec3.pdf)    | [Pattern Recognition and Machine Learning Ch.1 & Ch.4](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf)<br> [Deep Learning Ch.6](https://www.deeplearningbook.org/)  |     |
| 6  | 12/6  | Neural Networks (NNs)            | Softmax Classifier, Neural networks       | [Lec4](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec4.pdf)    | [Deep Learning Ch.6](https://www.deeplearningbook.org/)   <br> [The Neural Network, A Visual Introduction](https://www.youtube.com/watch?v=UOvPeC8WOt8&t=20s) <br> [Why are neural networks so effective?](https://www.youtube.com/watch?v=-at7SLoVK_I&t=732s)                                                                                                                                                                                                                        | [HW1](https://github.com/spml-sut/spml-sut.github.io/raw/main/HWs/SMPL_HW1.zip)    |
| 7  | 12/8  | Neural Networks (NNs)            | Neural networks            | [Lec5](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec5.pdf)    | [Deep Learning Ch.6](https://www.deeplearningbook.org/)   <br> [Backpropagation for a Linear Layer](https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html)    <br> [What is backpropagation really doing?](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)                                                                                                                                                                                                                         |     |
| 8  | 12/13 | Neural Networks (NNs)                | Forward and backward propagation      | [Lec6](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec6.pdf)    | [Deep Learning Ch.9](https://www.deeplearningbook.org/)                                                                                                                                                                                                                    |     |
| 9  | 12/15 | Convolutional NNs                | Convolutional Neural Networks (CNNs)                            | [Lec7](https://drive.google.com/file/d/1Irbdsef7PLnzMMR4WZzcCP2iCpsRj9CS/view?usp=sharing)    | [Deep Learning Ch.9](https://www.deeplearningbook.org/)                                                                                                                                                                                                                                     |     |
| 10 | 12/20 | Regularization <br> Optimization | **Mini-Exam**, Batch Normalization, CNNs Architecture     | [Lec8](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec8.pdf)    | [Dive into Deep Learning Ch. 8](http://d2l.ai/)                                                                                                                                                                                                              |     |
| 11 | 12/22 | Adversarial Examples             | CNNs Architecture, AE Generating Methods                      | [Lec9](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec9.pdf)    | [Dive into Deep Learning Ch. 8](http://d2l.ai/)  <br> [Intriguing Properties of Neural Networks](https://arxiv.org/abs/1312.6199)                                                                                                             |  [HW2](https://github.com/spml-sut/spml-sut.github.io/raw/main/HWs/SPML_HW2.zip)   |                                                                                                 
| 12 | 1/14  | Adversarial Examples             | AE Generating Methods                      | [Lec10](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec10.pdf)   | [Intriguing Properties of Neural Networks](https://arxiv.org/abs/1312.6199) |     |
| 13 | 1/19  | Adversarial Examples             | AE Generating Methods                              | [Lec11](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec11.pdf)   |  [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)                  |     |
| 14 | 1/21  | Adversarial Examples             |      AE Generating Methods                  | [Lec12](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec12.pdf)   |   [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)                                                           | [HW3](https://github.com/spml-sut/spml-sut.github.io/raw/main/HWs/SPML_HW3.zip)    |
| 15 | 1/26  | Adversarial Examples             |       AE Generating Methods                   | [Lec13](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec13.pdf)   |       [Universal Adversarial Perturbations](https://arxiv.org/abs/1610.08401) <br> [Adversarial Patch](https://arxiv.org/abs/1712.09665)                                                                                                                                                                                         |     |
| 16 | 1/28  | Adversarial Examples             |     Defenses Against AEs                            | [Lec14](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec14.pdf)   |    [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083) <br> [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf)                                                                             |     |
| 17 | 2/2   | Public Holiday                   |                                            |         |                                                                                                                                                                                                                                                                                                       |     |
| 18 | 2/4   |     Adversarial Examples                  |      Defenses Against AEs                                      |   [Lec15](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec15.pdf)      |    [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/pdf/1902.02918) <br> [Provably robust deep learning via adversarially trained smoothed classifiers](https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf)   |     |
| - | 2/7   | **Mid-term Exam**                    |                                            |         |                                                                                                                                                                                                                                                                                                                                                                               |     |
| 19 | 2/9   |     Adversarial Examples                  |      Defenses Against AEs                                      |   [Lec16](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec16.pdf)     |    [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/pdf/1902.02918) <br> [Provably robust deep learning via adversarially trained smoothed classifiers](https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf)   |     |
| 20 | 2/11   |      Adversarial Examples             |       Black-box AEs                                     |   [Lec17](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec17.pdf)      |  [Practical Black-Box Attacks against Machine Learning](https://www.cs.purdue.edu/homes/bb/2020-fall-cs590bb/docs/at/attacks-against-machine-learning.pdf) <br>        [ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models](https://dl.acm.org/doi/pdf/10.1145/3128572.3140448)    | |
| 21 | 2/16   | Presentation                     |            Student presentation                                |    [Pres1-1](https://raw.githubusercontent.com/spml-sut/spml-sut.github.io/main/Presentation/Pres1.pdf)     |   [Adversarial Training for Free!](https://arxiv.org/pdf/1904.12843.pdf) <br>        [Data Augmentation Can Improve Robustness](https://proceedings.neurips.cc/paper/2021/file/fb4c48608ce8825b558ccf07169a3421-Paper.pdf) <br>   [Adversarial Examples for Malware Detection](https://link.springer.com/chapter/10.1007/978-3-319-66399-9_4) <br>   [Perceptual Adversarial Robustness: Defense Against Unseen Threat Models](https://arxiv.org/abs/2006.12655) <br>                                                                                                                                                                                                                                                                                                                                                                       |     |
| 22 | 2/18   |      Adversarial Examples             |       Black-box AEs   - Data Poisoning                                 |   [Lec18](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec18.pdf)    |    [Black-box Adversarial Attacks with Limited Queries and Information](https://arxiv.org/pdf/1804.08598) <br> [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)  <br> [Clean-Label Backdoor Attacks](https://people.csail.mit.edu/madry/lab/cleanlabel.pdf)   | [HW4](https://github.com/spml-sut/spml-sut.github.io/raw/main/HWs/SPML_HW4.zip) |
| 23 | 2/23   | Presentation                     |                         Student presentation                   |          [Pres1-2](https://raw.githubusercontent.com/spml-sut/spml-sut.github.io/main/Presentation/Pres2.pdf)     |   [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175) <br>        [Intriguing Properties of Vision Transformers](https://proceedings.neurips.cc/paper/2021/file/c404a5adbf90e09631678b13b05d9d7a-Paper.pdf) <br>   [Audio Adversarial Examples: Targeted Attacks on Speech-to-Text](https://arxiv.org/abs/1801.01944) <br>   [Increasing Confidence in Adversarial Robustness Evaluations](https://arxiv.org/abs/2206.13991) <br>                                                                                                                                                                                                                                                                                                                                                                               |     |
| 24 | 2/25  | Poisoning                        | Poisoning                                  |  [Lec19](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec19.pdf)  | [Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792) <br> [Deep Partition Aggregation: Provable Defense against General Poisoning Attacks](https://arxiv.org/pdf/2006.14768)                                                                                                                                     |     |
| 25 | 2/30  | Model Extraction                 | ME Attacks                                 | [Lec20](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec20.pdf)   | [High Accuracy and High Fidelity Extraction of Neural Networks](https://arxiv.org/abs/1909.01838)  <br> [Knockoff Nets: Stealing Functionality of Black-Box Models](https://arxiv.org/abs/1812.02766) |     |
| 26 | 3/1  | Model Extraction   - Privacy              | ME Defenses - Privacy Risks                              | [Lec21](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec21.pdf)   |  [Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring](https://arxiv.org/pdf/1802.04633)     <br>  [Membership Inference Attacks against Machine Learning Models](https://arxiv.org/abs/1610.05820)                   |     |                                                                                                       
| 27 | 3/6  | Privacy                          | Privacy Risks                       | [Lec22](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec22.pdf)   | [Passive and Active White-box Inference Attacks against Centralized and Federated Learning](https://arxiv.org/abs/1812.00910) <br> [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)                                                                                                                    |  [HW5](https://github.com/spml-sut/spml-sut.github.io/raw/main/HWs/SPML_HW5.zip)   |
| 28 | 3/8  | Privacy                          | Differential Privacy                       | [Lec23](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec23.pdf)   | [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)                                                                                                                                                                                                                                                        |     |
| 29 | 3/13   | Privacy                          | Privacy-preserving DL                      | [Lec24](https://github.com/spml-sut/spml-sut.github.io/raw/main/Lectures/Lec24.pdf)   | [Deep Learning with Differential Privacy](https://arxiv.org/abs/1607.00133) <br> [Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data](https://arxiv.org/abs/1610.05755)                                                                                                                                                                                                       |   [HW6](https://github.com/spml-sut/spml-sut.github.io/raw/main/HWs/SPML_HW6.zip)   |
| 30 | 4/10   |     Presentation            |       Student Presentation                               |   [Pres2]()    |  [Reverse-Engineering Deep ReLU Networks]() <be> [ Trojaning Attack on Neural Networks]() <be> [Poisoning and Backdooring Contrastive Learning]() <be> [Extracting Training Data from Large Language Models]() <be> [Deep Leakage from Gradients]() <be> [Label-Only Membership Inference Attacks](https://arxiv.org/abs/2007.14321) <be> [Renyi Differential Privacy]() <be> [Large Language Models Can Be Strong Differentially Private Learners]()   |  |
