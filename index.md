<center><b>Security and Privacy in Machine Learning</b></center>
<center>Sharif University of Technology, Iran</center>
<center>CE Department</center>
<center>Spring 2023</center>


&nbsp;&nbsp;&nbsp;


_Welcome_ to the public page for the course on Security and Privacy in Machine Learning (SPML). The main objectives of the course are to introduce students to the principles of security and privacy in machine learning. The students become familiar with the vulnerabilities of machine learning in the training and inference phases and the methods to improve the robustness and privacy of machine learning models.



**Course Logistics**

   * **Time:** Sat. & Mon. 13:30 - 15:00
   * **Location:** CE-202 & [vc.sharif.edu/ch/amsadeghzadeh](https://vc.sharif.edu/ch/amsadeghzadeh)
   * **Contact:** Announcements and all course-related questions will happen on the [Quera](https://quera.org/course/add_to_course/course/13164/) forum. 
     * All official announcements and communication will happen over Quera.
     * For external enquiries, emergencies, or personal matters that you don't wish to put in a private post, you can email me at amsadeghzadeh_at_gmail.com



**Instructor**

&nbsp;&nbsp;&nbsp;_Amir Mahdi Sadeghzadeh_  
&nbsp;&nbsp;&nbsp;Office: CE-501 (DNSL)  
&nbsp;&nbsp;&nbsp;Office Hours: By appointment (through Email)  
&nbsp;&nbsp;&nbsp;Email: [amsadeghzadeh_at_gmail.com](mailto:amsadeghzadeh_at_gmail.com)  
&nbsp;&nbsp;&nbsp;URL: [amsadeghzadeh.github.io](https://amsadeghzadeh.github.io)  



**Course Staff**

* _Mahdi Ghaznavi_ (Head Course Assistant) - Email: [ghaznavi.mahdi_at_gmail.com](mailto:ghaznavi.mahdi_at_gmail.com)
* _Zeinab Golgooni_ (Course Assistant) - Email: [z.golgooni_at_gmail.com](mailto:z.golgooni_at_gmail.com)
* _Elahe Farshadfar_ (Course Assistant) - Email: [elahefarshadfar1377_at_gmail.com](mailto:elahefarshadfar1377_at_gmail.com)
* _Hamid Dashtbani_ (Course Assistant) - Email: 



**Course Pages** 

* [spml-sut.github.io](spml-sut.github.io) -> Course information, syllabus, and materials.
* [Quera](https://quera.org/course/add_to_course/course/13164/) (Get the password from course staff) -> Announcements, assignments, and all course-related questions.



**Main References** 

The main references for the course are many research papers in top-tier conferences and journals in computer security (SP, CCS, Usenix Security, EuroSP) and machine learning (NeurIPS, ICLR, ICML, CVPR, ECCV). Three following books are used for presenting background topics in machine learning and deep learning in
the first part of the course.

-   [Christopher M. Bishop, *Pattern Recognition and Machine Learning*,
    Springer,
    2006.](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

-   [Ian Goodfellow, *Deep Learning*, MIT Press,
    2016.](https://www.deeplearningbook.org/)

-   [Aston Zhang, Dive into Deep Learning, 2020 ](http://d2l.ai/)



**Grading Policy**

Assignments (30%), Mid-term (and Mini-exam) (20%), Papers review and presentation(20%), and Final (30%).



**Course Policy**

-   This course considers topics involving personal and public privacy
    and security. As part of this investigation we will cover
    technologies whose abuse may infringe on the rights of others. As an
    instructor, I rely on the ethical use of these technologies.
    Unethical use may include circumvention of existing security or
    privacy measurements for any purpose, or the dissemination,
    promotion, or exploitation of vulnerabilities of these services.
    Exceptions to these guidelines may occur in the process of reporting
    vulnerabilities through public and authoritative channels. Any
    activity outside the letter or spirit of these guidelines will be
    reported to the proper authorities and may result in dismissal from
    the class. When in doubt, please contact the instructor for advice. **Do not**
    undertake any action which could be perceived as technology misuse
    anywhere and/or under any circumstances unless you have received
    explicit permission from Dr. Sadeghzadeh.



**Academic Honesty** 

[Sharif CE Department Honor Code](https://wiki.ce.sharif.edu/%D8%A2%DB%8C%DB%8C%D9%86_%D9%86%D8%A7%D9%85%D9%87/%D8%A2%D8%AF%D8%A7%D8%A8_%D9%86%D8%A7%D9%85%D9%87_%D8%A7%D9%86%D8%AC%D8%A7%D9%85_%D8%AA%D9%85%D8%B1%DB%8C%D9%86_%D9%87%D8%A7%DB%8C_%D8%AF%D8%B1%D8%B3%DB%8C) (please read it carefully!)



**Homework Submission**

Submit your answers in .pdf or .zip file in course page on quera website, with the following format:
HW[HW#]-[FamilyName]-[std#] (For example HW3-Hoseini-400100111)



**Late Policy**

* All students have 14 free late days for the assignments.
* You may use up to 5 late days per assignment with no penalty.
* Once you have exhausted your free late days, we will deduct a late penalty of 20% per additional late day.


&nbsp;&nbsp;&nbsp;

&nbsp;&nbsp;&nbsp;


| # | Date  | Topic             | Content                                    | Lecture | Reading                                                                                                                                                                                                                                                                               | HWs |
|---|-------|-------------------|--------------------------------------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----|
| 1 | 11/17 | Course Intro.     | The scope and contents of the course       | Lec1    | [Towards the Science of Security and Privacy in Machine Learning.](https://arxiv.org/abs/1611.03814)                                                                                                                                                                           |     |
| 2 | 11/22 | Public Holiday           |                                            |         |                                                                                                                                                                                                                                                                                       |     |
| 3 | 11/24 | Machine Learning  | ML Intro., Perceptron, Logistic regression | Lec2    | [Pattern Recognition and Machine Learning Ch.1 \& Ch.4](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf) <br> [Deep Learning Ch.5](https://www.deeplearningbook.org/) |     |
| 4 | 11/29 | Public Holiday           |                                            |         |                                                                                                                                                                                                                                                                                       |     |
| 5 | 12/1  | Linear Classifier | Gradient descent, Softmax Classifier       | Lec3    | [Pattern Recognition and Machine Learning Ch.1 \& Ch.4](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf)<br> [Deep Learning Ch.6](https://www.deeplearningbook.org/)  |     |
| 6  | 12/6  | Neural Networks (NNs)            | Neural networks, Computational Graph       | Lec4    | [Deep Learning Ch.6](https://www.deeplearningbook.org/)                                                                                                                                                                                                                          |     |
| 7  | 12/8  | Neural Networks (NNs)            | Forward and backward propagation           | Lec5    | [Deep Learning Ch.6](https://www.deeplearningbook.org/)                                                                                                                                                                                                                          |     |
| 8  | 12/13 | Convolutional NNs                | Convolutional Neural Networks (CNNs)       | Lec6    | [Deep Learning Ch.9](https://www.deeplearningbook.org/)                                                                                                                                                                                                                          |     |
| 9  | 12/15 | Convolutional NNs                | CNNs Architecture                          | Lec7    | [Dive into Deep Learning Ch. 7](http://d2l.ai/)                                                                                                                                                                                                                                 |     |
| 10 | 12/20 | Regularization <br> Optimization | Momentum, Batch Normalization, Dropout     | Lec8    | [Deep Learning Ch.7 and Ch.8](https://www.deeplearningbook.org/)                                                                                                                                                                                                                 |     |
| 11 | 12/22 | Adversarial Examples             | AE Generating Methods                      | Lec9    | [Intriguing Properties of Neural Networks](https://arxiv.org/abs/1312.6199)<br> [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)                                                                                                             |     |                                                                                                 
| 12 | 1/14  | Adversarial Examples             | AE Generating Methods                      | Lec10   | [Distillation as a Defense to Adversarial Perturbations Against DNNs](https://arxiv.org/abs/1511.04508) <br> [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644) <br> [Adversarial Examples in the Physical World](https://arxiv.org/pdf/1607.02533.pdf) |     |
| 13 | 1/19  | Adversarial Examples             | AE Properties                              | Lec11   | [Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples](https://arxiv.org/abs/1605.07277) <br> [Universal Adversarial Perturbations](https://arxiv.org/abs/1610.08401) <br> [Adversarial Patch](https://arxiv.org/abs/1712.09665)                |     |
| 14 | 1/21  | Adversarial Examples             | Defenses Against AEs                       | Lec12   | [Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods](https://arxiv.org/pdf/1705.07263.pdf) <br> [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)                                                                 |     |
| 15 | 1/26  | Adversarial Examples             | Defenses Against AEs                       | Lec13   | [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/pdf/1902.02918)                                                                                                                                                                                                    |     |
| 16 | 1/28  | Adversarial Examples             | Black-box AEs                              | Lec14   | [Black-box Adversarial Attacks with Limited Queries and Information](https://arxiv.org/pdf/1804.08598) <br> [Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors](https://arxiv.org/pdf/1807.07978)                                                                      |     |
| 17 | 2/2   | Public Holiday                   |                                            |         |                                                                                                                                                                                                                                                                                                       |     |
| 18 | 2/4   | Presentation                     |                                            |         |                                                                                                                                                                                                                                                                                                                                                                               |     |
| 19 | 2/9   | Mid-term Exam                    |                                            |         |                                                                                                                                                                                                                                                                                                                                                                               |     |
| 20 | 2/11  | Poisoning                        | Poisoning                                  | Lec15   | [Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792) <br> [Deep Partition Aggregation: Provable Defense against General Poisoning Attacks](https://arxiv.org/pdf/2006.14768)                                                                                                                                     |     |
| 21 | 2/16  | Model Extraction                 | ME Attacks                                 | Lec16   | [High Accuracy and High Fidelity Extraction of Neural Networks](https://arxiv.org/abs/1909.01838) <br> [Practical Black-Box Attacks against Machine Learning](https://www.cs.purdue.edu/homes/bb/2020-fall-cs590bb/docs/at/attacks-against-machine-learning.pdf) <br> [Knockoff Nets: Stealing Functionality of Black-Box Models](https://arxiv.org/abs/1812.02766) |     |
| 22 | 2/18  | Model Extraction                 | ME Defenses                                | Lec17   | [PRADA: Protecting Against DNN Model Stealing Attacks](https://arxiv.org/abs/1805.02628)<br> [Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks](https://arxiv.org/abs/1906.10908) <br> [Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring](https://arxiv.org/pdf/1802.04633)                        |     |
| 23 | 2/23  | Privacy                          | Membership Inference Attacks               | Lec18   | [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://arxiv.org/abs/1802.08232) <br> [Membership Inference Attacks against Machine Learning Models](https://arxiv.org/abs/1610.05820)                                                                                                                                        |     |
| 24 | 2/25  | Privacy                          | Differential Privacy                       | Lec19   | [Passive and Active White-box Inference Attacks against Centralized and Federated Learning](https://arxiv.org/abs/1812.00910) <br> [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)                                                                                                                    |     |
| 25 | 2/30  | Privacy                          | Differential Privacy                       | Lec20   | [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)                                                                                                                                                                                                                                                        |     |
| 26 | 3/1   | Privacy                          | Privacy-preserving DL                      | Lec21   | [Deep Learning with Differential Privacy](https://arxiv.org/abs/1607.00133) <br> [Scalable Private Learning With PATE](https://openreview.net/pdf?id=rkZB1XbRZ)                                                                                                                                                                                                        |     |
| 27 | 3/6   | Fairness and Ethics              | Fairness and Ethics                        | Lec22   | [Fairness Through Awareness](https://arxiv.org/abs/1104.3913)                                                                                                                                                                                                                                                                                                            |     |
| 28 | 3/8   | Presentation                     |                                            |         |                                                                                                                                                                                                                                                                                                                                                                               |     |
